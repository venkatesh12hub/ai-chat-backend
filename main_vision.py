#!/usr/bin/env python3
"""
Quick setup script for AI Chat with Vision & Voice
Installs necessary models and tests all features
"""

import asyncio
import httpx
import subprocess
import sys
import os

async def check_ollama():
    """Check if Ollama is running"""
    print("ğŸ” Checking Ollama...")
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get("http://localhost:11434/api/tags")
            if response.status_code == 200:
                print("âœ… Ollama is running!")
                return True, response.json().get("models", [])
    except:
        pass
    print("âŒ Ollama is not running!")
    print("   Start it with: ollama serve")
    return False, []

def install_llava():
    """Install llava vision model"""
    print("\nğŸ“¥ Installing Llava vision model...")
    print("   This may take 5-10 minutes (downloading ~4.5GB)")
    
    try:
        result = subprocess.run(
            ["ollama", "pull", "llava"],
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            print("âœ… Llava installed successfully!")
            return True
        else:
            print(f"âŒ Failed to install llava: {result.stderr}")
            return False
    except FileNotFoundError:
        print("âŒ 'ollama' command not found!")
        print("   Please install Ollama from: https://ollama.ai/download")
        return False
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False

def install_python_deps():
    """Install Python dependencies"""
    print("\nğŸ“¦ Installing Python dependencies...")
    
    try:
        result = subprocess.run(
            [sys.executable, "-m", "pip", "install", "-r", "requirements_enhanced.txt"],
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            print("âœ… Python dependencies installed!")
            return True
        else:
            # Try individual install
            deps = ["fastapi", "uvicorn[standard]", "httpx", "pydantic", "python-multipart"]
            for dep in deps:
                subprocess.run([sys.executable, "-m", "pip", "install", dep], 
                             capture_output=True)
            print("âœ… Python dependencies installed!")
            return True
    except Exception as e:
        print(f"âš ï¸  Warning: {e}")
        print("   You may need to install manually:")
        print("   pip install fastapi uvicorn httpx pydantic python-multipart")
        return False

async def test_vision():
    """Test if vision model works"""
    print("\nğŸ§ª Testing vision capabilities...")
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get("http://localhost:11434/api/tags")
            if response.status_code == 200:
                models = response.json().get("models", [])
                has_vision = any("llava" in m["name"] for m in models)
                
                if has_vision:
                    print("âœ… Vision model (llava) is available!")
                    return True
                else:
                    print("âŒ Vision model not found")
                    return False
    except:
        return False

def check_browser_features():
    """Check browser feature support"""
    print("\nğŸŒ Browser Feature Support:")
    print("   Voice Input: Chrome âœ…, Edge âœ…, Safari âœ…, Firefox âŒ")
    print("   Voice Output: All modern browsers âœ…")
    print("   Image Upload: All browsers âœ…")
    print("\n   ğŸ’¡ For best experience, use Chrome or Edge")

async def main():
    print("=" * 70)
    print("ğŸš€ AI Chat with Vision & Voice - Setup")
    print("=" * 70)
    
    # Step 1: Check Ollama
    ollama_running, models = await check_ollama()
    if not ollama_running:
        print("\nâš ï¸  Please start Ollama first, then run this script again.")
        return
    
    # Step 2: Check for existing models
    print(f"\nğŸ“¦ Found {len(models)} models:")
    has_text_model = False
    has_vision_model = False
    
    for model in models:
        name = model.get("name", "unknown")
        print(f"   - {name}")
        if "qwen" in name or "llama" in name or "mistral" in name:
            has_text_model = True
        if "llava" in name or "vision" in name:
            has_vision_model = True
    
    # Step 3: Install models if needed
    if not has_text_model:
        print("\nğŸ“¥ Installing text model (qwen2.5:0.5b)...")
        subprocess.run(["ollama", "pull", "qwen2.5:0.5b"], capture_output=True)
        print("âœ… Text model installed!")
    
    if not has_vision_model:
        print("\nâ“ Llava vision model not found.")
        response = input("   Install llava for image analysis? (y/n): ").lower()
        if response == 'y':
            install_llava()
        else:
            print("   âš ï¸  Skipping vision model (image features won't work)")
    else:
        print("âœ… Vision model already installed!")
    
    # Step 4: Install Python dependencies
    install_python_deps()
    
    # Step 5: Test vision
    if has_vision_model or await test_vision():
        print("\nâœ… All vision features ready!")
    
    # Step 6: Browser features info
    check_browser_features()
    
    # Summary
    print("\n" + "=" * 70)
    print("ğŸ“Š SETUP SUMMARY")
    print("=" * 70)
    print(f"Ollama:           {'âœ… Running' if ollama_running else 'âŒ Not running'}")
    print(f"Text Model:       {'âœ… Ready' if has_text_model else 'âŒ Missing'}")
    print(f"Vision Model:     {'âœ… Ready' if has_vision_model else 'âš ï¸  Optional'}")
    print(f"Python Deps:      âœ… Installed")
    print("=" * 70)
    
    print("\nğŸ‰ Setup Complete!")
    print("\nğŸ“ Next Steps:")
    print("   1. Start backend:  python main_with_vision.py")
    print("   2. Open browser:   index_enhanced.html")
    print("   3. Try features:")
    print("      - ğŸ“· Click camera icon to upload image")
    print("      - ğŸ¤ Click mic icon for voice input")
    print("      - ğŸ”Š Click speaker to enable voice output")
    print("\nğŸ’¡ Tip: Read VISION_VOICE_GUIDE.md for detailed instructions")
    print()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Setup cancelled by user")
        sys.exit(1)